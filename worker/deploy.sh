#!/bin/bash

# Check the required environment variables.
: ${PROJECT:?"Please set PROJECT to your Google Cloud Project ID"}
: ${ML_ENGINE_TOPIC:?"Please set ML_ENGINE_TOPIC to a PubSub topic"}
: ${GOOGLE_APPLICATION_CREDENTIALS:?"Please set GOOGLE_APPLICATION_CREDENTIALS to point to the path/to/your/credentials.json"}

# Copy your credentials to upload to the server.
cp $GOOGLE_APPLICATION_CREDENTIALS worker/credentials.json

# Generate the workers's Dockerfile with the environment variables.
cat > server/app.yaml <<EOF
# This file is autogenerated, please change from deploy.sh

FROM tensorflow/tensorflow:latest-py3

COPY requirements.txt /
RUN pip install -U -r /requirements.txt

ENV WORKDIR /app
WORKDIR ${WORKDIR}
COPY . ${WORKDIR}

ENV GOOGLE_APPLICATION_CREDENTIALS credentials.json
ENV PROJECT $PROJECT
ENV ML_ENGINE_TOPIC $ML_ENGINE_TOPIC

CMD ["python", "main.py"]
EOF

# Build and push the docker container to Container Registry.
DEPLOYMENT=workers
IMAGE=gcr.io/$PROJECT/$DEPLOYMENT:latest

docker build -t $IMAGE worker/
docker push $IMAGE

# Create the workers Deployment to run the service and autoscale it in Kubernetes.
kubectl delete deployment $DEPLOYMENT
kubectl run $DEPLOYMENT --image $IMAGE
kubectl autoscale deployment $DEPLOYMENT \
  --min 1 --max 50 --cpu-percent 80
